# Rik's Weekly Notes

**Quote of the Week**
"The bottleneck is no longer compute—it's memory. And whoever solves memory, wins the AI race." — Industry consensus, 2025

---

## A) Technical Deep Dive - CPU-GPU Convergence

### New Confluence Blog Published
- Just published: **"The CPU-GPU Convergence: How NVIDIA's Strategic Moves Are Reshaping AI Infrastructure"**
- Key insights: AI workloads are memory-bound, not compute-bound
- The unified memory architecture can cut our infrastructure costs by 50%
- **Action**: Please read and share with your teams - this impacts our GB200 deployment strategy

### Breaking: NVIDIA Rubin CPX Announcement
- New GPU class with 128GB GDDR7 (cost-efficient vs HBM)
- 3x faster attention than GB300 NVL72
- Purpose-built for million-token contexts
- Available: End of 2026
- **What this means for us**: Validates our unified memory approach in production

---

## B) OCI Momentum - We're Leading the Pack

### Scale Update
- **131,072 Blackwell GPUs** now available - world's largest AI supercomputer
- 65,536 H200 GPUs in GA
- GB200 NVL72 with 129.6 TB/s aggregate bandwidth
- **Competitive advantage**: Bare metal (no virtualization tax) + scale = unbeatable

### Customer Wins
- OpenAI, Meta, NVIDIA partnerships expanding
- AMD MI355X GPUs coming to OCI
- **RPO Update**: $130B total backlog - we're building for explosive growth

---

## C) Technical Excellence - Memory Architecture Revolution

### CXL Production Ready
- CXL 3.1 controllers now shipping
- Remote DRAM access: 5-10µs (game-changing latency)
- Real test: 512GB DDR5 + 512GB CXL = same performance as 1TB DRAM at fraction of cost
- **Implication**: This enables our rack-scale memory pools (up to 100TB)

### The New Memory Hierarchy
```
L1/L2 Cache     → nanoseconds
GPU HBM         → sub-microsecond
Local CPU DRAM  → microseconds
Remote CPU DRAM → 5-10 microseconds (via CXL/Enfabrica)
SSD/Storage     → milliseconds
```

---

## D) Operations Excellence

### This Week's Focus
- GB200 readiness: Review memory tiering strategies
- Capacity planning: Factor in 50% efficiency gains from unified memory
- Patent opportunity: Unified memory optimization techniques (reach out for guidance)

### Action Items
- **Engineers**: Experiment with memory tiering in dev environments
- **Architects**: Update designs for CXL-ready systems
- **SREs**: Plan monitoring for cross-fabric memory metrics

---

## E) Team & Growth

### Recognition
- Shoutout to teams preparing GB200 infrastructure
- Welcome new members joining the GPU Control Plane team
- Congrats to those completing NVIDIA certifications

### Hiring Push Continues
- @shihsun still actively hiring for GPU infrastructure roles
- **Ask**: Amplify on LinkedIn - your network matters
- Focus areas: Memory systems, CXL expertise, distributed computing

---

## F) Looking Ahead

### Strategic Questions
1. How do we optimize workload placement for unified memory?
2. Should we prioritize memory bandwidth or capacity in our configs?
3. What's our CXL adoption timeline?

---

**Easter Egg**: A single Llama-3 405B model needs 810GB in FP16. With unified memory, we can serve it on 4 GPUs instead of 8. That's not optimization—that's revolution.

---

**To:** @gpucpninjas
**CC:** @sukochar @smosborn @jlherman @shihsun @sudhir

*Remember: We're not just managing infrastructure anymore—we're architecting the nervous system of AI. The memory wall is becoming a doorway, and OCI is walking through it first.*
